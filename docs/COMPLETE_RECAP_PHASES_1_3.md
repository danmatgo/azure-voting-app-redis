# üéì Recapitulaci√≥n Completa: De Cero a Producci√≥n en Azure
## Todo lo que hiciste en las Fases 1-3 explicado paso a paso

---

# üìñ La Historia Completa

## ¬øQu√© construimos?

Imagina que llegaste a trabajar el primer d√≠a y te dicen: *"Necesitamos una aplicaci√≥n web de votaci√≥n que corra en la nube, sea escalable, y se pueda actualizar sin downtime."*

Esto es exactamente lo que hiciste. Vamos a recorrer cada paso como si te lo explicaran en un caf√©.

---

## üèóÔ∏è FASE 1: La Fundaci√≥n (Terraform)

### ¬øQu√© hiciste?
Creaste toda la infraestructura de Azure usando c√≥digo en lugar de clics en el portal.

### La analog√≠a simple:
> Imagina que vas a construir una casa. Antes de poner ladrillos, necesitas:
> - Un terreno (Resource Group)
> - Conexi√≥n el√©ctrica y agua (Virtual Network)
> - Una bodega para materiales (Container Registry)
> - El taller donde trabajar√°n los obreros (Kubernetes Cluster)

### Los archivos que creaste:

```
terraform/
‚îú‚îÄ‚îÄ providers.tf   ‚Üí "Dile a Terraform que vamos a usar Azure"
‚îú‚îÄ‚îÄ variables.tf   ‚Üí "Los valores configurables (regi√≥n, tama√±o de VMs, etc.)"
‚îú‚îÄ‚îÄ main.tf        ‚Üí "La receta: qu√© recursos crear"
‚îî‚îÄ‚îÄ outputs.tf     ‚Üí "Al terminar, mu√©strame estos datos importantes"
```

### Cada recurso explicado:

| Recurso | ¬øQu√© es? | Analog√≠a simple |
|---------|----------|-----------------|
| **Resource Group** | Carpeta que contiene todo | Una caja donde pones todo tu proyecto |
| **Virtual Network** | Red privada aislada | Tu barrio cerrado privado |
| **Subnet** | Subdivisi√≥n de la red | Una calle espec√≠fica dentro del barrio |
| **NSG** | Firewall | El guardia de seguridad que decide qui√©n entra |
| **ACR** | Registro de im√°genes Docker | Un √°lbum de fotos de tus aplicaciones empaquetadas |
| **AKS** | Cluster de Kubernetes | La f√°brica donde corren tus aplicaciones |
| **Log Analytics** | Sistema de logs | Las c√°maras de seguridad que graban todo |

### El secreto de la seguridad: Managed Identity

```
Problema: ¬øC√≥mo hace el cluster (AKS) para descargar im√°genes del registro (ACR)?

Opci√≥n mala: Guardar un password ‚Üí Puede filtrarse
Opci√≥n buena: Managed Identity ‚Üí Azure le da una "tarjeta de acceso" al AKS
```

**Lo que escribiste en Terraform:**
```hcl
identity {
  type = "SystemAssigned"  # Azure crea una identidad autom√°tica
}

# Luego le diste permiso de "solo lectura" al ACR
resource "azurerm_role_assignment" "aks_acr_pull" {
  role_definition_name = "AcrPull"  # Solo puede descargar, no subir ni borrar
}
```

### Comandos que ejecutaste:
```bash
terraform init     # "Descarga los plugins necesarios"
terraform plan     # "Mu√©strame qu√© vas a hacer (sin hacerlo)"
terraform apply    # "Ahora s√≠, cr√©alo todo"
terraform destroy  # "Borra todo cuando termine"
```

---

## üê≥ FASE 2: Empaquetar la App (Docker)

### ¬øQu√© hiciste?
Tomaste la aplicaci√≥n Python y la empaquetaste en una "caja" que puede correr en cualquier lugar.

### La analog√≠a simple:
> Imagina que tienes una receta de cocina que funciona perfectamente en tu casa. 
> Docker es como empacar todos los ingredientes, utensilios, y hasta la cocina 
> en una caja que puedes llevar a cualquier lado y funcionar√° igual.

### El Dockerfile que creaste:

```dockerfile
# ETAPA 1: El taller de empaque
FROM python:3.11-slim as builder     # "Usa Python como base"
RUN pip install dependencias...      # "Instala las librer√≠as"

# ETAPA 2: El producto final (limpio y peque√±o)
FROM python:3.11-slim                # "Empezar limpio"
COPY --from=builder las_librerias    # "Trae solo lo necesario del taller"
USER appuser                         # "Corre como usuario sin permisos"
```

### ¬øPor qu√© dos etapas (multi-stage)?

```
Sin multi-stage:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Python + gcc + cache + deps ‚îÇ ‚Üí 500MB de imagen
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Con multi-stage:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Python + gcc    ‚îÇ ‚îÄ‚îÄ‚ñ∂ ‚îÇ Python + deps ‚îÇ ‚Üí 150MB de imagen
‚îÇ (se descarta)   ‚îÇ     ‚îÇ (final)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Beneficios:**
- Imagen m√°s peque√±a = m√°s r√°pido de descargar
- Sin herramientas de build = menos superficie de ataque

### ¬øPor qu√© usuario non-root?

```
Con root:       Si hackean la app ‚Üí Control total del container
Con appuser:    Si hackean la app ‚Üí Solo pueden tocar /app, nada del sistema
```

### Comandos que ejecutaste:
```bash
docker build -t mi-app .              # "Construye la imagen"
docker tag mi-app acr.io/mi-app:v1    # "Ponle una etiqueta con la direcci√≥n del ACR"
docker push acr.io/mi-app:v1          # "S√∫bela al registro"
```

---

## ‚ò∏Ô∏è FASE 3: Orquestar Todo (Kubernetes)

### ¬øQu√© hiciste?
Le dijiste a Kubernetes: "Quiero 2 copias de mi app corriendo, que se reinicien si fallan, y que escalen si hay mucho tr√°fico".

### La analog√≠a simple:
> Kubernetes es como un gerente de restaurante muy eficiente:
> - "Siempre quiero 2 meseros trabajando"
> - "Si uno se enferma, contrata otro inmediatamente"
> - "Si llegan m√°s clientes, contrata meseros temporales"
> - "Cuando pase la hora pico, reduce el equipo"

### Los conceptos clave:

#### Pod
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       POD       ‚îÇ  ‚Üê La unidad m√°s peque√±a
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Container ‚îÇ  ‚îÇ  ‚Üê Tu app corre aqu√≠
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ   IP ef√≠mera    ‚îÇ  ‚Üê Cambia si el pod muere
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
**Problema**: Si el pod muere, nadie lo recrea. Por eso usamos Deployment.

#### Deployment
```
         DEPLOYMENT
              ‚îÇ
              ‚îÇ "Mant√©n 2 r√©plicas siempre"
              ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Pod 1 ‚îÇ          ‚îÇ Pod 2 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ                   ‚îÇ
    ‚îÇ Si muere ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ                   ‚îÇ
    ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Pod 1 ‚îÇ (nuevo)  ‚îÇ Pod 2 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Service
```
Usuario ‚Üí IP p√∫blica ‚Üí SERVICE ‚Üí distribuye tr√°fico ‚Üí Pod 1
                                                    ‚Üí Pod 2
```
**Problema que resuelve**: Los pods tienen IPs que cambian. El Service da una IP/DNS estable.

#### Rolling Update (lo que configuraste)
```
Estado inicial:     [Pod v1] [Pod v1]    ‚Üê 2 pods versi√≥n vieja

Paso 1:            [Pod v1] [Pod v1] [Pod v2]   ‚Üê Crea 1 nuevo (maxSurge: 1)

Paso 2:            [Pod v1] [Pod v2] [Pod v2]   ‚Üê Mata 1 viejo cuando nuevo est√° listo

Paso 3:            [Pod v2] [Pod v2]            ‚Üê Todos actualizados, zero downtime
```

**Tu configuraci√≥n:**
```yaml
maxSurge: 1        # M√°ximo 1 pod extra durante update
maxUnavailable: 0  # Siempre mantener 2 disponibles
```

#### Probes (los chequeos de salud)

```
                 KUBERNETES
                     ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                ‚îÇ                ‚îÇ
    ‚ñº                ‚ñº                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LIVENESS‚îÇ    ‚îÇREADINESS‚îÇ    ‚îÇ STARTUP ‚îÇ
‚îÇ         ‚îÇ    ‚îÇ         ‚îÇ    ‚îÇ(opcional)‚îÇ
‚îÇ"¬øVive?" ‚îÇ    ‚îÇ"¬øListo?"‚îÇ    ‚îÇ"¬øArranc√≥?‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ              ‚îÇ
     ‚ñº              ‚ñº
 Si falla:      Si falla:
 REINICIA      REMUEVE DEL
 EL POD        LOAD BALANCER
```

**Ejemplo real:**
1. Pod arranca, pero est√° cargando datos (30 segundos)
2. Liveness: ‚úÖ "Est√° vivo" 
3. Readiness: ‚ùå "No est√° listo para tr√°fico"
4. Service NO env√≠a tr√°fico a este pod
5. 30 segundos despu√©s, Readiness: ‚úÖ
6. Service ahora S√ç env√≠a tr√°fico

---

## üîß Los Problemas que Encontraste y C√≥mo los Resolviste

### Problema 1: Puerto incorrecto

**S√≠ntoma**: Pods en CrashLoopBackOff o probes fallando

**Lo que pas√≥**:
```
La gu√≠a dec√≠a:     containerPort: 80
Tu app escuchaba:  puerto 8080
```

**Soluci√≥n**: Cambiaste a `containerPort: 8080` en el Deployment

**Lecci√≥n**: Siempre verificar en qu√© puerto escucha tu app real.

### Problema 2: Imagen no se actualiza

**S√≠ntoma**: Pusheaste nueva imagen pero el pod usa la vieja

**Lo que pas√≥**: Docker cachea im√°genes con tag `latest`

**Soluci√≥n**: Agregaste `imagePullPolicy: Always`

**Lecci√≥n**: En producci√≥n usa tags con SHA o versi√≥n, no `latest`.

---

## üè¢ ¬øC√≥mo Cambia Esto en un Proyecto Real Enterprise (EPAM)?

### Lo que hiciste vs. Lo que encontrar√°s

| Aspecto | Tu Ejercicio | Proyecto Enterprise Real |
|---------|--------------|-------------------------|
| **N√∫mero de microservicios** | 2 (frontend + redis) | 20-100+ microservicios |
| **Ambientes** | 1 (dev) | 4+ (dev, staging, QA, prod) |
| **Clusters** | 1 AKS | M√∫ltiples clusters, multi-regi√≥n |
| **Networking** | VNet simple | Hub-spoke, Private Link, Firewall, VPN |
| **Secrets** | Env vars simples | Azure Key Vault, External Secrets Operator |
| **CI/CD** | GitHub Actions b√°sico | Pipelines con gates, approval, blue-green |
| **Monitoring** | Container Insights | Prometheus + Grafana + alertas complejas |
| **Seguridad** | NSG b√°sico | Pod Security Policies, OPA/Gatekeeper, mTLS |

### Pero los conceptos son los mismos

```
Tu ejercicio:                    Proyecto enterprise:
                                 
Deployment simple         ‚Üí      Deployment con m√°s config
Service LoadBalancer      ‚Üí      Ingress Controller + WAF
ConfigMap b√°sico          ‚Üí      External Config + Feature Flags
HPA por CPU               ‚Üí      KEDA con m√∫ltiples triggers
kubectl apply manual      ‚Üí      GitOps con ArgoCD/Flux
```

### Lo que te prepara:

| Concepto que aprendiste | C√≥mo escala a enterprise |
|------------------------|--------------------------|
| Pods, Deployments | Igual, pero m√°s de ellos |
| Services, Labels | Igual, pero con m√°s convenciones de naming |
| Probes | Igual, pero con endpoints dedicados de health |
| HPA | Se vuelve KEDA para scaling basado en eventos |
| kubectl | Se vuelve GitOps (ArgoCD) |
| Terraform | Igual, pero con m√≥dulos y workspaces |
| Multi-stage Docker | Igual, pero con base images corporativas |

---

## üí™ Por Qu√© Puedes Afrontarlo con Confianza

### 1. Los fundamentos son los mismos
```
Aprendiste:                 Enterprise usa:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Pod                    ‚Üí    Muchos pods
Deployment             ‚Üí    Muchos deployments
Service                ‚Üí    Muchos services + Ingress
Terraform              ‚Üí    Terraform + m√≥dulos
Docker                 ‚Üí    Docker + registry scanning
```

### 2. La complejidad es aditiva, no diferente
```
Tu proyecto:          Enterprise:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
1 cluster        +    Multi-cluster management
1 VNet           +    Hub-spoke topology
ConfigMap        +    Key Vault integration
HPA              +    KEDA + cluster autoscaler
kubectl          +    ArgoCD + GitOps
```

### 3. Lo que cambia es principalmente:
- **Escala** (m√°s de todo)
- **Procesos** (approvals, PRs, documentation)
- **Seguridad** (m√°s capas)
- **Observabilidad** (m√°s m√©tricas)

### 4. Respuestas para la entrevista:

**"¬øHas trabajado con clusters multi-regi√≥n?"**
> "En mi experiencia directa trabaj√© con clusters single-region, pero entiendo la arquitectura: Azure Traffic Manager o Front Door para routing global, clusters independientes con GitOps para sync, y bases de datos con geo-replication. Los conceptos de Kubernetes son los mismos, la complejidad est√° en el networking y el state management."

**"¬øQu√© har√≠as si un pod falla en producci√≥n a las 3am?"**
> "Primero verifico los alerts en el monitoring - CPU, memoria, restarts. Luego kubectl describe pod para ver eventos, kubectl logs para ver errores. Si es cr√≠tico, puedo hacer rollback inmediato con kubectl rollout undo. Mientras tanto, el Deployment mantiene r√©plicas healthy sirviendo tr√°fico."

**"¬øC√≥mo manejar√≠as secrets en un proyecto grande?"**
> "No guardar√≠a secrets en ConfigMaps ni en Git. Usar√≠a Azure Key Vault integrado con AKS usando el CSI driver o External Secrets Operator. Los pods referencian secrets por nombre, y el operator los sincroniza autom√°ticamente. Para rotaci√≥n, el driver puede recargar secrets sin reiniciar pods."

---

## üìã Resumen Visual: El Flujo Completo que Dominaste

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           TU FLUJO DEVOPS                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                         ‚îÇ
‚îÇ   üìù C√ìDIGO                                                            ‚îÇ
‚îÇ      ‚îÇ                                                                  ‚îÇ
‚îÇ      ‚ñº                                                                  ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                      ‚îÇ
‚îÇ   ‚îÇ  Terraform  ‚îÇ terraform apply                                      ‚îÇ
‚îÇ   ‚îÇ  (IaC)      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ                                   ‚îÇ
‚îÇ                                    ‚ñº                                   ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ                    ‚îÇ         AZURE                 ‚îÇ                    ‚îÇ
‚îÇ                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ                    ‚îÇ
‚îÇ                    ‚îÇ  ‚îÇ RG  ‚îÇ ‚îÇ ACR ‚îÇ ‚îÇ AKS ‚îÇ    ‚îÇ                    ‚îÇ
‚îÇ                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚ñ≤‚îÄ‚îÄ‚îò    ‚îÇ                    ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ                                  ‚îÇ       ‚îÇ                             ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ       ‚îÇ                             ‚îÇ
‚îÇ   ‚îÇ  Dockerfile ‚îÇ docker push    ‚îÇ       ‚îÇ kubectl apply              ‚îÇ
‚îÇ   ‚îÇ  (Container)‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ                             ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ                             ‚îÇ
‚îÇ                                          ‚îÇ                             ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ                             ‚îÇ
‚îÇ   ‚îÇ  K8s YAML   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             ‚îÇ
‚îÇ   ‚îÇ  (Manifests)‚îÇ                                                      ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                      ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ                           USUARIOS                                     ‚îÇ
‚îÇ                              ‚îÇ                                         ‚îÇ
‚îÇ                              ‚ñº                                         ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                 ‚îÇ
‚îÇ                    ‚îÇ  LoadBalancer   ‚îÇ ‚Üê IP P√∫blica                   ‚îÇ
‚îÇ                    ‚îÇ      :80        ‚îÇ                                 ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                 ‚îÇ
‚îÇ                             ‚îÇ                                          ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                 ‚îÇ
‚îÇ                    ‚ñº                 ‚ñº                                 ‚îÇ
‚îÇ               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                            ‚îÇ
‚îÇ               ‚îÇ Pod 1   ‚îÇ      ‚îÇ Pod 2   ‚îÇ                            ‚îÇ
‚îÇ               ‚îÇ :8080   ‚îÇ      ‚îÇ :8080   ‚îÇ                            ‚îÇ
‚îÇ               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îÇ
‚îÇ                    ‚îÇ                ‚îÇ                                  ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                  ‚îÇ
‚îÇ                            ‚ñº                                           ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                     ‚îÇ
‚îÇ                    ‚îÇ   Redis     ‚îÇ                                     ‚îÇ
‚îÇ                    ‚îÇ  (backend)  ‚îÇ                                     ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                     ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéØ Lo Que Puedes Decir con Confianza en la Entrevista

> "Tengo experiencia pr√°ctica implementando pipelines DevOps end-to-end. Uso Terraform para IaC - Resource Groups, networking con VNet y NSG, AKS con managed identity para autenticaci√≥n Zero Trust hacia ACR. 
>
> Para containerizaci√≥n, implemento multi-stage Dockerfiles para reducir tama√±o de imagen y superficie de ataque, con usuarios non-root por seguridad.
>
> En Kubernetes, configuro Deployments con rolling updates para zero downtime, probes de liveness y readiness para self-healing, y HPA para autoscaling basado en m√©tricas. Los services internos usan ClusterIP, expongo externamente con LoadBalancer.
>
> Cuando hay problemas, uso kubectl describe y logs para diagnosticar. He manejado ImagePullBackOff verificando permisos ACR, y CrashLoopBackOff debuggeando logs del container.
>
> S√© que en enterprise esto escala a m√°s microservicios, GitOps con ArgoCD, y security layers adicionales, pero los fundamentos que manejo son la base de todo."

---

## ‚úÖ Checklist Final de Conocimientos

### Terraform
- [x] S√© qu√© hace cada archivo (providers, variables, main, outputs)
- [x] Entiendo el flujo: init ‚Üí plan ‚Üí apply ‚Üí destroy
- [x] Puedo explicar Managed Identity vs passwords
- [x] S√© por qu√© es importante el state file

### Docker
- [x] Entiendo multi-stage build y su beneficio
- [x] S√© por qu√© usar non-root user
- [x] Entiendo la diferencia entre build y runtime
- [x] Puedo explicar layers y cache

### Kubernetes
- [x] S√© la diferencia: Pod vs Deployment vs Service
- [x] Entiendo rolling update y sus par√°metros
- [x] Puedo explicar liveness vs readiness probes
- [x] S√© debuggear con kubectl describe y logs
- [x] Entiendo ClusterIP vs LoadBalancer
- [x] Puedo explicar HPA y autoscaling

### Troubleshooting
- [x] S√© diagnosticar ImagePullBackOff
- [x] S√© diagnosticar CrashLoopBackOff
- [x] S√© verificar eventos con kubectl get events
- [x] Puedo hacer rollback con kubectl rollout undo
