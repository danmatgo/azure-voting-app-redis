# ğŸ§  GUÃA MAESTRA: DevSecOps End-to-End
## Entendimiento Profundo de Infraestructura, Containers, Kubernetes y CI/CD

> **PropÃ³sito**: Entender el POR QUÃ‰ de cada decisiÃ³n tÃ©cnica, no solo el cÃ³mo.

---

# ğŸ“– TABLA DE CONTENIDOS

1. [La Arquitectura Completa](#la-arquitectura-completa)
2. [Fase 1: Terraform - Por quÃ© IaC importa](#fase-1-terraform)
3. [Fase 2: Contenedores - MÃ¡s que solo Docker](#fase-2-contenedores)
4. [Fase 3: Kubernetes - OrquestaciÃ³n real](#fase-3-kubernetes)
5. [Mejoras Enterprise](#mejoras-enterprise)
6. [Fase 4: CI/CD - AutomatizaciÃ³n inteligente](#fase-4-cicd)
7. [Fase 5: DevSecOps - Seguridad integrada](#fase-5-devsecops)
8. [Problemas y Soluciones](#problemas-y-soluciones)
9. [CÃ³mo explicar todo en una entrevista](#como-explicar-en-entrevista)

---

# ğŸ—ï¸ LA ARQUITECTURA COMPLETA

## Â¿QuÃ© construimos?

Una aplicaciÃ³n de votaciÃ³n con arquitectura de microservicios:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AZURE CLOUD                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    AZURE KUBERNETES SERVICE                   â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚
â”‚  â”‚  â”‚    FRONTEND     â”‚           â”‚      REDIS      â”‚           â”‚   â”‚
â”‚  â”‚  â”‚   (Python/Flask)â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚    (Cache DB)   â”‚           â”‚   â”‚
â”‚  â”‚  â”‚                 â”‚           â”‚                 â”‚           â”‚   â”‚
â”‚  â”‚  â”‚  Port 8080      â”‚           â”‚    Port 6379    â”‚           â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚
â”‚  â”‚           â–²                                                   â”‚   â”‚
â”‚  â”‚           â”‚ ClusterIP                                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚   â”‚
â”‚  â”‚  â”‚  LoadBalancer   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€ Internet                        â”‚   â”‚
â”‚  â”‚  â”‚    (Port 80)    â”‚                                          â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚     ACR      â”‚    â”‚   Key Vault  â”‚    â”‚  Log Analyticsâ”‚         â”‚
â”‚  â”‚  (ImÃ¡genes)  â”‚    â”‚  (Secretos)  â”‚    â”‚  (Monitoring) â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## El flujo de un usuario

```
1. Usuario abre http://IP_PUBLICA
2. Load Balancer recibe la peticiÃ³n
3. Rutea al pod del frontend (hay varios por HA)
4. Frontend consulta/actualiza votos en Redis
5. Frontend renderiza HTML con los resultados
6. Respuesta vuelve al usuario
```

---

# ğŸ”§ FASE 1: TERRAFORM

## Â¿Por quÃ© Infrastructure as Code?

Antes de IaC, la infraestructura se creaba manualmente en el portal de Azure. Problemas:

| Sin IaC | Con IaC (Terraform) |
|---------|---------------------|
| "Funciona en mi cuenta" | Reproducible en cualquier cuenta |
| DocumentaciÃ³n desactualizada | El cÃ³digo ES la documentaciÃ³n |
| Cambios no rastreables | Git history de cada cambio |
| Rollback manual y arriesgado | `terraform apply` de versiÃ³n anterior |
| Horas configurando manualmente | Minutos ejecutando cÃ³digo |

## Estructura de archivos Terraform

```
terraform/
â”œâ”€â”€ providers.tf      # DÃ³nde y cÃ³mo conectarse
â”œâ”€â”€ variables.tf      # Inputs configurables
â”œâ”€â”€ main.tf           # Recursos a crear
â”œâ”€â”€ outputs.tf        # Valores de salida
â””â”€â”€ environments/
    â”œâ”€â”€ dev.tfvars    # Valores para desarrollo
    â””â”€â”€ prod.tfvars   # Valores para producciÃ³n
```

### providers.tf - La conexiÃ³n

```hcl
terraform {
  required_version = ">= 1.0"
  
  backend "azurerm" {
    # DÃ³nde guardar el estado
    resource_group_name  = "tfstate-rg"
    storage_account_name = "tfstatevoting2390"
    container_name       = "tfstate"
    key                  = "votingapp-dev.tfstate"
  }
  
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.0"  # ~> significa >= 3.0.0 y < 4.0.0
    }
  }
}
```

**Â¿Por quÃ© `~> 3.0`?**: Permite actualizaciones menores (3.1, 3.2) que son backwards compatible, pero bloquea 4.0 que podrÃ­a tener breaking changes.

### variables.tf - ParametrizaciÃ³n

```hcl
variable "environment" {
  description = "Ambiente de despliegue"
  type        = string
  
  validation {
    condition     = contains(["dev", "staging", "prod"], var.environment)
    error_message = "Ambiente debe ser dev, staging, o prod."
  }
}

variable "aks_node_vm_size" {
  description = "TamaÃ±o de VM para nodos AKS"
  type        = string
  default     = "Standard_D2as_v4"
}
```

**Â¿Por quÃ© validaciones?**: Previene errores costosos. Si alguien escribe "produccion" en vez de "prod", Terraform falla antes de crear recursos incorrectos.

### main.tf - Los recursos

```hcl
locals {
  name_prefix = "${var.project_name}-${var.environment}"
  common_tags = merge(var.tags, {
    Environment = var.environment
  })
}

# Resource Group - Contenedor lÃ³gico de recursos
resource "azurerm_resource_group" "main" {
  name     = "${local.name_prefix}-rg"
  location = var.location
  tags     = local.common_tags
}

# Container Registry - AlmacÃ©n de imÃ¡genes Docker
resource "azurerm_container_registry" "main" {
  name                = "${var.project_name}${var.environment}acr"
  resource_group_name = azurerm_resource_group.main.name
  location            = azurerm_resource_group.main.location
  sku                 = "Basic"
  admin_enabled       = false  # Â¡Importante! Usar Managed Identity, no admin
  tags                = local.common_tags
}
```

**Â¿Por quÃ© `admin_enabled = false`?**: La autenticaciÃ³n con admin user y password es insegura. Usamos Managed Identity que no requiere credenciales.

### El AKS Cluster

```hcl
resource "azurerm_kubernetes_cluster" "main" {
  name                = "${local.name_prefix}-aks"
  location            = azurerm_resource_group.main.location
  resource_group_name = azurerm_resource_group.main.name
  dns_prefix          = "${local.name_prefix}-aks"
  kubernetes_version  = "1.32.0"

  default_node_pool {
    name                = "system"
    vm_size             = var.aks_node_vm_size
    vnet_subnet_id      = azurerm_subnet.aks.id
    enable_auto_scaling = var.aks_enable_autoscaling
    node_count          = var.aks_enable_autoscaling ? null : var.aks_node_count
    min_count           = var.aks_enable_autoscaling ? var.aks_min_nodes : null
    max_count           = var.aks_enable_autoscaling ? var.aks_max_nodes : null
  }

  identity {
    type = "SystemAssigned"  # AKS crea su propia identidad
  }

  network_profile {
    network_plugin = "kubenet"   # MÃ¡s simple, suficiente para mayorÃ­a de casos
    network_policy = "calico"    # Habilita Network Policies
    pod_cidr       = "10.244.0.0/16"
    service_cidr   = "10.0.2.0/24"
    dns_service_ip = "10.0.2.10"
  }

  oms_agent {
    log_analytics_workspace_id = azurerm_log_analytics_workspace.main.id
  }
}
```

**Decisiones de diseÃ±o**:
- **System-assigned Managed Identity**: Azure crea y maneja la identidad automÃ¡ticamente
- **Kubenet vs Azure CNI**: Kubenet es mÃ¡s simple y usa menos IPs. Azure CNI da cada pod una IP de la VNet (mÃ¡s complejo pero necesario para ciertas integraciones)
- **Calico**: Network Policy engine que permite reglas de firewall entre pods

### Role Assignment - Permisos

```hcl
resource "azurerm_role_assignment" "aks_acr_pull" {
  scope                = azurerm_container_registry.main.id
  role_definition_name = "AcrPull"
  principal_id         = azurerm_kubernetes_cluster.main.kubelet_identity[0].object_id
}
```

**Â¿Por quÃ© esto?**: El AKS necesita descargar imÃ¡genes del ACR. En lugar de usar credenciales, le asignamos un rol que le da permiso. Es el principio de **Least Privilege**: solo puede leer (Pull), no escribir (Push).

---

# ğŸ“¦ FASE 2: CONTENEDORES

## Â¿Por quÃ© contenedores?

```
PROBLEMA CLÃSICO:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Mi MÃ¡quina     â”‚     â”‚   Servidor      â”‚
â”‚                 â”‚     â”‚                 â”‚
â”‚  Python 3.11    â”‚     â”‚  Python 3.8     â”‚
â”‚  Flask 2.3      â”‚ â•â•â–¶ â”‚  Flask 2.0      â”‚
â”‚  Redis 4.5      â”‚     â”‚  Redis ???      â”‚
â”‚                 â”‚     â”‚                 â”‚
â”‚  "Funciona!"    â”‚     â”‚  "No funciona!" â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SOLUCIÃ“N CON CONTENEDORES:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            CONTAINER IMAGE              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Python 3.11 + Flask 2.3 + App    â”‚ â”‚
â”‚  â”‚  Exactamente igual en todos lados â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼               â–¼               â–¼
     Mi PC          Staging          Prod
   (funciona)      (funciona)     (funciona)
```

## El Dockerfile explicado

```dockerfile
# === ETAPA 1: BUILD ===
FROM python:3.11-slim AS builder

WORKDIR /app

# Copiar SOLO requirements primero (optimizaciÃ³n de cache)
COPY azure-vote/requirements.txt .

# Instalar dependencias en carpeta especÃ­fica
RUN pip install --no-cache-dir --prefix=/install -r requirements.txt

# === ETAPA 2: RUNTIME ===
FROM python:3.11-slim

# Metadatos de la imagen
LABEL maintainer="Daniel Matapi" \
      version="1.0" \
      description="Azure Voting App Frontend"

WORKDIR /app

# Crear usuario no-root ANTES de copiar archivos
RUN useradd --create-home --shell /bin/bash appuser

# Copiar dependencias desde builder
COPY --from=builder /install /usr/local

# Copiar cÃ³digo de la aplicaciÃ³n
COPY azure-vote/ .

# Cambiar ownership y usuario
RUN chown -R appuser:appuser /app
USER appuser

# Puerto que expone la app
EXPOSE 8080

# Health check - K8s y Docker pueden verificar que la app estÃ¡ viva
HEALTHCHECK --interval=30s --timeout=5s --start-period=5s --retries=3 \
    CMD python -c "import urllib.request; urllib.request.urlopen('http://localhost:8080/')" || exit 1

# Comando para iniciar la app
CMD ["python", "main.py"]
```

### Â¿Por quÃ© Multi-Stage Build?

```
SIN MULTI-STAGE:                    CON MULTI-STAGE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Image final        â”‚              â”‚ Build stage        â”‚
â”‚                    â”‚              â”‚ (se descarta)      â”‚
â”‚ - GCC, make, etc   â”‚              â”‚ - GCC, make, etc   â”‚
â”‚ - headers          â”‚              â”‚ - headers          â”‚
â”‚ - pip cache        â”‚              â”‚ - pip cache        â”‚
â”‚ - app + deps       â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                    â”‚                       â”‚
â”‚ TAMAÃ‘O: 1.2 GB     â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ Runtime stage      â”‚
                                    â”‚ (imagen final)     â”‚
                                    â”‚                    â”‚
                                    â”‚ - python slim      â”‚
                                    â”‚ - app + deps       â”‚
                                    â”‚                    â”‚
                                    â”‚ TAMAÃ‘O: 180 MB     â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Â¿Por quÃ© usuario no-root?

```
CON ROOT:                           SIN ROOT:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Container          â”‚              â”‚ Container          â”‚
â”‚ USER: root         â”‚              â”‚ USER: appuser      â”‚
â”‚                    â”‚              â”‚                    â”‚
â”‚ Si atacante entra: â”‚              â”‚ Si atacante entra: â”‚
â”‚ - Acceso total     â”‚              â”‚ - Solo /app        â”‚
â”‚ - Puede escapar    â”‚              â”‚ - No puede escapar â”‚
â”‚ - DaÃ±o ilimitado   â”‚              â”‚ - DaÃ±o limitado    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## requirements.txt - Versiones pinned

```txt
Flask==2.3.3
redis==4.6.0
gunicorn==21.2.0
Werkzeug==2.3.7
```

**Â¿Por quÃ© versiones exactas?**: 
- `Flask>=2.0` podrÃ­a instalar Flask 3.0 maÃ±ana y romper la app
- Versiones exactas garantizan reproducibilidad
- En producciÃ³n siempre quieres saber EXACTAMENTE quÃ© tienes

---

# â˜¸ï¸ FASE 3: KUBERNETES

## Â¿Por quÃ© Kubernetes?

Docker solo corre contenedores. Kubernetes los **orquesta**:

| Docker Solo | Kubernetes |
|-------------|------------|
| 1 contenedor en 1 servidor | N contenedores en N servidores |
| Se muere â†’ se queda muerto | Se muere â†’ se recrea automÃ¡ticamente |
| Escalar manualmente | Escalar automÃ¡ticamente basado en mÃ©tricas |
| Balanceo manual | Service Discovery y Load Balancing built-in |
| Updates arriesgados | Rolling updates sin downtime |

## Namespace - Aislamiento lÃ³gico

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: voting-app
  labels:
    app: voting-app
    environment: dev
```

**Â¿Por quÃ©?**: Evita colisiones de nombres. Puedes tener `voting-app/frontend` y `otra-app/frontend` sin conflictos.

## ConfigMap - ConfiguraciÃ³n externa

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: voting-app-config
  namespace: voting-app
data:
  TITLE: "Azure Voting App"
  VOTE1VALUE: "Cats"
  VOTE2VALUE: "Dogs"
```

**Â¿Por quÃ© ConfigMap?**: Separar configuraciÃ³n del cÃ³digo. Puedes cambiar los valores de votaciÃ³n sin reconstruir la imagen.

## Deployment - El controlador de pods

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: voting-app
spec:
  replicas: 2
  
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0    # Nunca menos de 2 pods
      maxSurge: 1          # MÃ¡ximo 3 pods temporalmente
  
  selector:
    matchLabels:
      app: frontend
  
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
        - name: frontend
          image: votingappdevacr.azurecr.io/azure-vote-front:latest
          imagePullPolicy: Always
          
          ports:
            - containerPort: 8080
          
          envFrom:
            - configMapRef:
                name: voting-app-config
          
          env:
            - name: REDIS
              value: "redis"
          
          resources:
            requests:
              cpu: 100m      # MÃ­nimo garantizado
              memory: 128Mi
            limits:
              cpu: 500m      # MÃ¡ximo permitido
              memory: 256Mi
          
          livenessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 15
            periodSeconds: 10
          
          readinessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 5
```

### ExplicaciÃ³n de cada secciÃ³n:

**replicas: 2**
- Siempre hay 2 pods corriendo â†’ Alta disponibilidad
- Si uno muere, el otro sigue sirviendo mientras se recrea

**RollingUpdate strategy**
```
Estado inicial:     [Pod1-v1] [Pod2-v1]
Creando nuevo:      [Pod1-v1] [Pod2-v1] [Pod3-v2]
Eliminando viejo:   [Pod2-v1] [Pod3-v2]
Creando nuevo:      [Pod2-v1] [Pod3-v2] [Pod4-v2]
Eliminando viejo:   [Pod3-v2] [Pod4-v2]
â†’ ZERO DOWNTIME
```

**resources (requests vs limits)**
```
requests: Lo que Kubernetes GARANTIZA
limits: Lo MÃXIMO que puede usar

Si pones limits muy bajos â†’ OOMKilled (Out of Memory)
Si pones requests muy altos â†’ No schedula en nodos pequeÃ±os
```

**livenessProbe vs readinessProbe**
```
livenessProbe:
- "Â¿EstÃ¡ vivo el proceso?"
- Si falla â†’ K8s MATA el pod y crea uno nuevo

readinessProbe:
- "Â¿EstÃ¡ listo para recibir trÃ¡fico?"
- Si falla â†’ K8s DEJA DE ENVIAR trÃ¡fico (pero no mata el pod)
```

## Service - Networking

```yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend
spec:
  type: LoadBalancer
  ports:
    - port: 80           # Puerto expuesto externamente
      targetPort: 8080   # Puerto del contenedor
  selector:
    app: frontend        # EnvÃ­a trÃ¡fico a pods con este label
```

**Tipos de Service**:
```
ClusterIP (default):
  Solo accesible dentro del cluster
  Ej: Redis no necesita acceso externo

NodePort:
  Abre un puerto en cada nodo
  Ãštil para testing, no para producciÃ³n

LoadBalancer:
  Crea un Load Balancer en la nube
  IP pÃºblica accesible desde internet
```

## HPA - Horizontal Pod Autoscaler

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: frontend-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: frontend
  
  minReplicas: 2
  maxReplicas: 10
  
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
```

**Â¿CÃ³mo funciona?**
```
CPU promedio > 70% â†’ Agregar pods (hasta 10)
CPU promedio < 70% â†’ Reducir pods (mÃ­nimo 2)

Ejemplo:
- 2 pods al 90% CPU â†’ HPA crea pod 3
- 3 pods al 60% CPU â†’ HPA mantiene 3
- 3 pods al 20% CPU â†’ HPA reduce a 2
```

---

# ğŸ¢ MEJORAS ENTERPRISE

## 1. Remote Backend

**Problema**: Estado de Terraform guardado localmente
```
Dev1: terraform apply â†’ crea RG
Dev2: terraform apply â†’ NO VE el RG â†’ CONFLICTO
```

**SoluciÃ³n**: Estado en Azure Storage con locking
```
Dev1: terraform apply â†’ lock â†’ crea RG â†’ unlock
Dev2: terraform apply â†’ WAIT (locked) â†’ ve cambios de Dev1
```

## 2. Kustomize

**Problema**: Copiar manifests para cada ambiente
```
k8s/dev/deployment.yaml   (100 lÃ­neas)
k8s/prod/deployment.yaml  (100 lÃ­neas, cambios mÃ­nimos)
â†’ Mantener 2 archivos casi iguales
```

**SoluciÃ³n**: Base + overlays
```
k8s/base/deployment.yaml  (100 lÃ­neas)
k8s/overlays/dev/kustomization.yaml (10 lÃ­neas de parches)
k8s/overlays/prod/kustomization.yaml (10 lÃ­neas de parches)
```

## 3. PodDisruptionBudget

**Problema**: Durante upgrade de nodos, K8s puede matar todos los pods
```
Node upgrade â†’ drain â†’ todos los pods de frontend mueren â†’ DOWNTIME
```

**SoluciÃ³n**: PDB garantiza mÃ­nimo disponible
```yaml
spec:
  minAvailable: 1  # Siempre al menos 1 pod vivo
```

---

# ğŸ”„ FASE 4: CI/CD

## La filosofÃ­a

```
ANTES (manual):
Dev â†’ git push â†’ esperar â†’ ir a servidor â†’ pull â†’ build â†’ test â†’ deploy
                           â†“
                    "OlvidÃ© correr los tests"
                    "El build fallÃ³ en producciÃ³n"

AHORA (CI/CD):
Dev â†’ git push â†’ [AUTOMÃTICO: test â†’ build â†’ scan â†’ deploy]
                           â†“
                    Feedback en minutos
                    Mismo proceso siempre
```

## OIDC - AutenticaciÃ³n sin secretos

**Problema con secrets**:
```
GitHub Secret: AZURE_PASSWORD=MiPasswordSuperSecreto123
                    â†“
Â¿QuiÃ©n tiene acceso? Â¿CuÃ¡ndo se rota? Â¿QuÃ© pasa si GitHub se compromete?
```

**SoluciÃ³n OIDC**:
```
GitHub: "Soy el repo X, branch Y, usuario Z"
Azure: *verifica la firma de GitHub*
Azure: "OK, aquÃ­ tienes un token vÃ¡lido por 15 minutos"
â†’ No hay password almacenado
â†’ Token temporal limita el daÃ±o si algo sale mal
```

## El Workflow explicado

```yaml
name: CI/CD Pipeline

on:
  push:
    branches: [master]
    paths: ['azure-vote/**', 'k8s/**']  # Solo si cambian estos archivos
  workflow_dispatch:  # Permite ejecutar manualmente

permissions:
  id-token: write    # Necesario para OIDC
  contents: read

jobs:
  build:
    runs-on: ubuntu-latest
    outputs:
      image-tag: ${{ steps.meta.outputs.tag }}  # Pasa el tag al job deploy
    
    steps:
      - uses: actions/checkout@v4
      
      # Generar tag Ãºnico basado en commit SHA
      - name: Set image tag
        id: meta
        run: |
          SHORT_SHA=$(echo ${{ github.sha }} | cut -c1-7)
          echo "tag=ACR/IMAGE:${SHORT_SHA}" >> $GITHUB_OUTPUT
      
      # Login a Azure usando OIDC
      - uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      
      # Build sin pushear (para escanear primero)
      - uses: docker/build-push-action@v5
        with:
          push: false
          load: true
          tags: ${{ steps.meta.outputs.tag }}
      
      # Escanear ANTES de pushear
      - uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ steps.meta.outputs.tag }}
          severity: 'CRITICAL,HIGH'
      
      # Solo pushea si el scan pasa
      - run: docker push ${{ steps.meta.outputs.tag }}

  deploy:
    needs: build  # Espera a que build termine
    if: github.ref == 'refs/heads/master'  # Solo en master
    
    steps:
      # Configurar kubectl
      - name: Setup Kubeconfig
        run: |
          echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > ~/.kube/config
      
      # Actualizar imagen y desplegar
      - name: Deploy
        run: |
          cd k8s/overlays/dev
          kustomize edit set image IMAGE=${{ needs.build.outputs.image-tag }}
          kubectl apply -k .
      
      # Verificar que el deploy fue exitoso
      - run: kubectl rollout status deployment/frontend -n voting-app
```

---

# ğŸ”’ FASE 5: DEVSECOPS

## Shift-Left Security

```
ANTES (seguridad al final):
Dev â†’ Build â†’ Test â†’ Deploy â†’ SCAN â†’ "Houston, tenemos problemas"
                                          â†“
                              Rollback, hotfix, caos

AHORA (shift-left):
Dev â†’ SCAN â†’ Build â†’ SCAN â†’ Test â†’ SCAN â†’ Deploy
       â†“
   Detectar temprano = Arreglar barato
```

## Trivy - Container Scanning

Escanea la imagen Docker buscando:
- CVEs en el sistema operativo base
- Vulnerabilidades en librerÃ­as (Flask, Redis, etc.)
- Configuraciones inseguras

```yaml
- uses: aquasecurity/trivy-action@master
  with:
    image-ref: ${{ steps.meta.outputs.tag }}
    exit-code: '0'     # Reportar sin bloquear
    severity: 'CRITICAL,HIGH'
```

**Â¿Por quÃ© exit-code 0?**: A veces hay vulnerabilidades en el OS base (Debian, Alpine) sin fix disponible. Bloquear el pipeline no soluciona el problema, pero sÃ­ lo documentamos.

## Dependabot - Dependency Scanning

AutomÃ¡ticamente:
1. Detecta dependencias desactualizadas
2. Verifica si tienen CVEs conocidos
3. Crea PRs con actualizaciones

## CodeQL - Static Analysis

Analiza el cÃ³digo Python buscando patrones inseguros:
- SQL Injection
- XSS
- Hardcoded credentials
- Path traversal

## Network Policies - Zero Trust Networking

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: redis-allow-frontend-only
spec:
  podSelector:
    matchLabels:
      app: redis
  policyTypes: [Ingress]
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: frontend
      ports:
        - port: 6379
```

**Â¿QuÃ© hace esto?**
```
SIN Network Policy:
  Cualquier pod â†’ puede hablar con â†’ Redis
  (un atacante en cualquier pod puede robar datos)

CON Network Policy:
  Solo frontend â†’ puede hablar con â†’ Redis
  Todo lo demÃ¡s â†’ BLOQUEADO
```

---

# ğŸ”§ PROBLEMAS Y SOLUCIONES

## 1. Error de permisos en Entra ID

**SÃ­ntoma**:
```
az ad app create --display-name "test"
ERROR: Insufficient privileges
```

**Causa**: La cuenta no tiene rol de Application Administrator o Global Admin.

**SoluciÃ³n**: En entornos enterprise, solicitar permisos elevados temporales, o usar una cuenta de servicio con los permisos necesarios por separado.

## 2. Trivy bloqueando el pipeline

**SÃ­ntoma**:
```
CRITICAL: libssl3 CVE-2024-XXXX
Pipeline: FAILED
```

**Causa**: Vulnerabilidad en la imagen base de Debian sin parche disponible.

**Soluciones aplicables**:
1. Cambiar a imagen base mÃ¡s segura (Alpine, Distroless)
2. Si no hay parche, documentar la excepciÃ³n y monitorear
3. Configurar Trivy para reportar sin bloquear (decisiÃ³n de riesgo aceptado)

## 3. Deployment fallando con imagen incorrecta

**SÃ­ntoma**:
```
ImagePullBackOff
```

**Causas comunes**:
1. Nombre de imagen mal escrito
2. AKS no tiene permisos para pull del ACR
3. La imagen no existe en el registry

**VerificaciÃ³n**:
```bash
# Â¿Existe la imagen?
az acr repository show-tags --name MYACR --repository IMAGE

# Â¿Tiene permisos el AKS?
az role assignment list --scope /subscriptions/X/resourceGroups/Y/providers/Microsoft.ContainerRegistry/registries/Z
```

## 4. Pod en CrashLoopBackOff

**SÃ­ntoma**: Pod reiniciÃ¡ndose constantemente

**DiagnÃ³stico**:
```bash
kubectl describe pod POD_NAME -n NAMESPACE
kubectl logs POD_NAME -n NAMESPACE --previous
```

**Causas comunes**:
1. AplicaciÃ³n crashea al iniciar (error de cÃ³digo)
2. Variable de entorno faltante
3. No puede conectar a dependencia (Redis)
4. Recursos insuficientes (OOMKilled)

## 5. Cross-account deployment

**SituaciÃ³n**: ACR en una cuenta, AKS en otra.

**SoluciÃ³n implementada**:
1. OIDC para autenticar contra la cuenta del ACR
2. Kubeconfig exportado como secret para acceder al AKS
3. El workflow usa ambos mÃ©todos en jobs separados

---

# ğŸ¤ CÃ“MO EXPLICAR EN ENTREVISTA

## Sobre IaC y Terraform

> "La infraestructura la manejo como cÃ³digo con Terraform. Uso remote backend en Azure Storage para el state compartido con locking, lo cual evita condiciones de carrera cuando mÃºltiples personas trabajan en la infraestructura. Para manejar ambientes, uso tfvars separados con un archivo de variables por ambiente, entonces el mismo cÃ³digo despliega a dev, staging o prod con diferentes configuraciones."

## Sobre Contenedores

> "Para containerizaciÃ³n uso multi-stage builds que reducen el tamaÃ±o de imagen significativamente - pasÃ© de mÃ¡s de un GB a menos de 200MB eliminando herramientas de build del runtime. Las imÃ¡genes corren con usuario no-root para limitar el blast radius en caso de compromiso. Versiono las dependencias exactas en requirements.txt para garantizar reproducibilidad."

## Sobre Kubernetes

> "Uso Deployments con rolling updates configurados para zero-downtime. Tengo liveness y readiness probes diferenciados - liveness para detectar si el proceso muriÃ³, readiness para control de trÃ¡fico durante startups lentos. Los recursos estÃ¡n definidos con requests y limits para evitar noisy neighbors. Para configuraciÃ³n uso ConfigMaps y para secretos, integraciÃ³n con Key Vault."

## Sobre CI/CD

> "El pipeline usa GitHub Actions con OIDC para autenticaciÃ³n contra Azure - no hay secrets de credenciales almacenados, solo tokens de corta duraciÃ³n. El flujo es: build de imagen, scan de vulnerabilidades con Trivy antes de push, y deploy a Kubernetes usando Kustomize para aplicar configuraciones especÃ­ficas del ambiente."

## Sobre Seguridad

> "Implemento shift-left security: escaneo de contenedores antes de push, anÃ¡lisis estÃ¡tico de cÃ³digo con CodeQL, y Dependabot para dependencias. En Kubernetes uso Network Policies para microsegmentaciÃ³n - por ejemplo, solo el frontend puede hablar con Redis. Los pods corren con security context hardened y PodDisruptionBudgets garantizan disponibilidad durante mantenimiento."

## Sobre troubleshooting

> "Para debugging en K8s mi flujo tÃ­pico es: kubectl get pods para ver estado, describe para eventos y condiciones, logs para errores de aplicaciÃ³n. Si necesito investigar mÃ¡s, hago exec al container o creo un pod de debug. Para problemas de red, verifico Network Policies y uso pods efÃ­meros con curl o nslookup."

---

# âœ… CHECKLIST DE CONOCIMIENTOS

| Tema | Â¿Puedo explicar el POR QUÃ‰? | Â¿Puedo resolver problemas? |
|------|----------------------------|---------------------------|
| Terraform state y locking | â˜ | â˜ |
| tfvars y ambientes | â˜ | â˜ |
| Multi-stage Dockerfile | â˜ | â˜ |
| Usuario no-root en containers | â˜ | â˜ |
| Deployment vs Pod | â˜ | â˜ |
| Rolling updates | â˜ | â˜ |
| Liveness vs Readiness probes | â˜ | â˜ |
| Requests vs Limits | â˜ | â˜ |
| Service types | â˜ | â˜ |
| HPA | â˜ | â˜ |
| Kustomize overlays | â˜ | â˜ |
| OIDC | â˜ | â˜ |
| Trivy y container scanning | â˜ | â˜ |
| Network Policies | â˜ | â˜ |

---

**Recuerda**: No memorizas comandos, entiendes sistemas. La entrevista busca gente que entiende el **por quÃ©**, no solo el **cÃ³mo**.
