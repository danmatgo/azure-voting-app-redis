# ğŸ§  GUÃA MAESTRA - PARTE 3
## Kubernetes - ExplicaciÃ³n profunda

---

# â˜¸ï¸ KUBERNETES - Â¿POR QUÃ‰ EXISTE?

## El problema que Docker NO resuelve

Docker es genial para empaquetar y correr UN contenedor. Pero en producciÃ³n:

| Pregunta | Docker solo | Necesitas algo mÃ¡s |
|----------|-------------|-------------------|
| Â¿QuÃ© pasa si un contenedor se cae? | Se queda caÃ­do | Algo que lo reinicie |
| Â¿CÃ³mo corro 10 copias? | Manualmente | Algo que lo maneje |
| Â¿CÃ³mo distribuyo trÃ¡fico entre copias? | Manualmente | Un load balancer |
| Â¿CÃ³mo actualizo sin downtime? | Bajar, actualizar, subir | Rolling updates |
| Â¿CÃ³mo escalo cuando hay mÃ¡s trÃ¡fico? | Adivinar y crear mÃ¡s | Autoscaling |

**Kubernetes resuelve todos estos problemas.**

## Conceptos fundamentales

### Cluster

Un cluster de Kubernetes es un grupo de mÃ¡quinas trabajando juntas.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     KUBERNETES CLUSTER                       â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   CONTROL PLANE    â”‚  â”‚       NODES        â”‚             â”‚
â”‚  â”‚   (El cerebro)     â”‚  â”‚  (Los trabajadores) â”‚            â”‚
â”‚  â”‚                    â”‚  â”‚                    â”‚             â”‚
â”‚  â”‚  - API Server      â”‚  â”‚  Node 1            â”‚             â”‚
â”‚  â”‚  - Scheduler       â”‚  â”‚  â”œâ”€â”€ Pod A         â”‚             â”‚
â”‚  â”‚  - Controller Mgr  â”‚  â”‚  â””â”€â”€ Pod B         â”‚             â”‚
â”‚  â”‚  - etcd            â”‚  â”‚                    â”‚             â”‚
â”‚  â”‚                    â”‚  â”‚  Node 2            â”‚             â”‚
â”‚  â”‚                    â”‚  â”‚  â”œâ”€â”€ Pod C         â”‚             â”‚
â”‚  â”‚                    â”‚  â”‚  â””â”€â”€ Pod D         â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Control Plane** (en AKS, Azure lo maneja por ti):
- **API Server**: El punto de entrada. Todo pasa por aquÃ­.
- **Scheduler**: Decide en quÃ© Node correr cada Pod.
- **Controller Manager**: Mantiene el estado deseado.
- **etcd**: Base de datos de todo el estado del cluster.

**Nodes** (tÃº pagas por estos):
- MÃ¡quinas virtuales que corren tus contenedores
- Cada Node tiene un agente (kubelet) que habla con el Control Plane

### Pod

La unidad mÃ¡s pequeÃ±a en Kubernetes. Un Pod contiene uno o mÃ¡s contenedores.

```
POD = Grupo de contenedores que:
â”œâ”€â”€ Comparten network namespace (misma IP)
â”œâ”€â”€ Comparten storage volumes
â”œâ”€â”€ Se schedulejan juntos en el mismo Node
â””â”€â”€ Se escalan juntos

CASO TÃPICO: 1 contenedor por Pod
CASO AVANZADO: App + sidecar (logging, proxy, etc.)
```

**Â¿Por quÃ© Pod y no Container directamente?**

El Pod agrega funcionalidades:
- Health checks (liveness, readiness)
- Restart policies
- Resource limits
- Volumes compartidos

### Deployment

Un Deployment maneja un grupo de Pods idÃ©nticos.

```
DEPLOYMENT "frontend"
â”‚
â”œâ”€â”€ ReplicaSet (maneja las rÃ©plicas)
â”‚   â”‚
â”‚   â”œâ”€â”€ Pod frontend-abc123
â”‚   â”œâ”€â”€ Pod frontend-def456
â”‚   â””â”€â”€ Pod frontend-ghi789
â”‚
â””â”€â”€ Strategy: RollingUpdate
```

**Â¿Por quÃ© no crear Pods directamente?**

1. Si un Pod muere, nadie lo recrea
2. No hay forma de tener mÃºltiples copias idÃ©nticas
3. No hay forma de actualizar sin downtime

El Deployment se encarga de todo esto.

### Service

Un Service da una direcciÃ³n estable a un grupo de Pods.

**El problema**: Los Pods son efÃ­meros. Su IP cambia cada vez que se recrean.

```
SIN SERVICE:
App â†’ Pod IP 10.244.0.5 â†’ Pod muere
App â†’ 10.244.0.5 ??? â†’ Error!

CON SERVICE:
App â†’ Service "redis" â†’ Cualquier Pod con label app=redis
                       (Kubernetes resuelve automÃ¡ticamente)
```

**Tipos de Service**:

```
ClusterIP (default):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CLUSTER                 â”‚
â”‚                                 â”‚
â”‚  App â”€â”€â”€â”€ ClusterIP â”€â”€â”€â”€ Pods  â”‚
â”‚           (solo interno)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Solo accesible dentro del cluster.
Uso: bases de datos, services internos.


NodePort:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CLUSTER                 â”‚
â”‚                                 â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NodePort â”€â”€â”€â”€â”€â”€â”€â”€ Pods        â”‚â—€â”€â”€â”€â”€â”€â”‚ Internet â”‚
â”‚  (puerto 30000-32767)           â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Abre un puerto en cada Node.
Uso: testing, acceso directo a nodes.


LoadBalancer:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CLUSTER                 â”‚      â”‚ Azure        â”‚
â”‚                                 â”‚â—€â”€â”€â”€â”€â”€â”‚ Load Balancerâ”‚â—€â”€â”€ Internet
â”‚  LoadBalancer â”€â”€â”€â”€ Pods        â”‚      â”‚ (IP pÃºblica) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Azure crea un Load Balancer real con IP pÃºblica.
Uso: aplicaciones web pÃºblicas.
```

---

# MANIFESTS DE KUBERNETES - LÃNEA POR LÃNEA

## namespace.yaml

```yaml
apiVersion: v1
```
**Â¿QuÃ© es?**: La versiÃ³n de la API de Kubernetes para este tipo de recurso.

Diferentes recursos tienen diferentes versiones:
- `v1`: Stable, para Pods, Services, etc.
- `apps/v1`: Para Deployments, DaemonSets, etc.
- `autoscaling/v2`: Para HPA

```yaml
kind: Namespace
```
**Â¿QuÃ© es?**: El tipo de recurso que estamos creando.

```yaml
metadata:
  name: voting-app
  labels:
    app: voting-app
    environment: dev
```
**`metadata`**: InformaciÃ³n SOBRE el recurso.
- `name`: CÃ³mo se llama (Ãºnico dentro del cluster)
- `labels`: Etiquetas clave-valor para organizar y seleccionar recursos

**Â¿Para quÃ© sirven los labels?**

Para seleccionar recursos. Ejemplos:
- "Dame todos los pods con label `app=frontend`"
- "Borra todo con label `environment=dev`"
- "Aplica polÃ­tica de red a pods con label `tier=backend`"

---

## configmap.yaml

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: voting-app-config
  namespace: voting-app
data:
  TITLE: "Azure Voting App"
  VOTE1VALUE: "Cats"
  VOTE2VALUE: "Dogs"
```

**Â¿QuÃ© es un ConfigMap?**

Un lugar para guardar configuraciÃ³n que no es secreta.

**Â¿Por quÃ© no hardcodear en el cÃ³digo?**

1. Cambiar config sin reconstruir imagen
2. Diferentes valores para diferentes ambientes
3. Separar configuraciÃ³n de cÃ³digo (buena prÃ¡ctica)

**Â¿CÃ³mo llega al contenedor?**

OpciÃ³n 1: Variables de entorno (lo que usamos):
```yaml
envFrom:
  - configMapRef:
      name: voting-app-config
# Result: TITLE=Azure Voting App, VOTE1VALUE=Cats, etc.
```

OpciÃ³n 2: Archivos montados:
```yaml
volumes:
  - name: config
    configMap:
      name: voting-app-config
# Result: Archivos en el filesystem del contenedor
```

---

## deployment.yaml - LÃ­nea por LÃ­nea

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: voting-app
  labels:
    app: frontend
```

La metadata del Deployment. `apps/v1` porque Deployments estÃ¡n en el grupo `apps`.

```yaml
spec:
```
**Â¿QuÃ© es spec?**: La especificaciÃ³n. Lo que QUEREMOS que exista.

```yaml
  replicas: 2
```
**Â¿QuÃ© significa?**: "Siempre quiero 2 Pods corriendo".

Si hay 0, Kubernetes crea 2.
Si hay 3, Kubernetes mata 1.
Si hay 2, perfecto.

```yaml
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
```

**Â¿QuÃ© es la estrategia?**: CÃ³mo actualizar cuando cambias la imagen.

**RollingUpdate**: Actualizar gradualmente, sin downtime.

```
Estado inicial:  [Pod1-v1] [Pod2-v1]
Crear nuevo:     [Pod1-v1] [Pod2-v1] [Pod3-v2]
Matar viejo:              [Pod2-v1] [Pod3-v2]
Crear nuevo:              [Pod2-v1] [Pod3-v2] [Pod4-v2]
Matar viejo:                        [Pod3-v2] [Pod4-v2]
Final:                              [Pod3-v2] [Pod4-v2]

â†’ En todo momento hubo al menos 2 pods sirviendo trÃ¡fico
â†’ ZERO DOWNTIME
```

**`maxUnavailable: 0`**: Nunca tener menos del nÃºmero deseado.
**`maxSurge: 1`**: MÃ¡ximo 1 Pod extra durante la transiciÃ³n.

Otra opciÃ³n es `Recreate`: mata todo y crea de nuevo. Tiene downtime pero es mÃ¡s simple.

```yaml
  selector:
    matchLabels:
      app: frontend
```

**Â¿QuÃ© hace selector?**: Define quÃ© Pods "pertenecen" a este Deployment.

El Deployment busca Pods con label `app: frontend` y los considera suyos.

**IMPORTANTE**: Esto TIENE que coincidir con los labels del template de abajo.

```yaml
  template:
    metadata:
      labels:
        app: frontend
```

**Â¿QuÃ© es template?**: La plantilla para crear Pods.

Los labels aquÃ­ DEBEN coincidir con el selector de arriba. Si no coinciden, el Deployment crea Pods pero no los reconoce como suyos.

```yaml
    spec:
      containers:
        - name: frontend
          image: votingappdevacr.azurecr.io/azure-vote-front:latest
          imagePullPolicy: Always
```

**`image`**: QuÃ© imagen Docker usar.

**`imagePullPolicy`**:
- `Always`: Siempre descargar del registry (asegura Ãºltima versiÃ³n)
- `IfNotPresent`: Solo descargar si no estÃ¡ en el Node
- `Never`: Nunca descargar (solo usar cachÃ© local)

**Â¿Por quÃ© `Always`?**: Si usamos tag `latest`, queremos la Ãºltima versiÃ³n. Si no, cada Node podrÃ­a tener versiones diferentes cacheadas.

En producciÃ³n se usan tags especÃ­ficos (ej: `v1.2.3` o el SHA del commit).

```yaml
          ports:
            - containerPort: 8080
```

**Â¿QuÃ© hace?**: Documenta que el contenedor escucha en puerto 8080.

**NO abre el puerto.** Es solo documentaciÃ³n. El puerto se expone via Service.

```yaml
          envFrom:
            - configMapRef:
                name: voting-app-config
```

Inyecta TODAS las claves del ConfigMap como variables de entorno.

Si el ConfigMap tiene:
```yaml
data:
  TITLE: "Azure Voting App"
  VOTE1VALUE: "Cats"
```

El contenedor tiene:
```
TITLE=Azure Voting App
VOTE1VALUE=Cats
```

```yaml
          env:
            - name: REDIS
              value: "redis"
```

Una variable de entorno especÃ­fica. Le dice a la app dÃ³nde estÃ¡ Redis.

**Â¿Por quÃ© "redis"?**: Es el nombre del Service de Redis. Kubernetes DNS resuelve "redis" a la IP del Service.

```yaml
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 500m
              memory: 256Mi
```

**Â¿QuÃ© son requests y limits?**

```
requests = Lo que Kubernetes GARANTIZA al Pod
limits = Lo MÃXIMO que el Pod puede usar

requests:
â”œâ”€â”€ Scheduler usa esto para decidir en quÃ© Node poner el Pod
â””â”€â”€ "Necesito al menos 100m CPU para funcionar"

limits:
â”œâ”€â”€ Si el Pod intenta usar mÃ¡s, Kubernetes lo throttlea (CPU) o lo mata (memoria)
â””â”€â”€ "Nunca dejes que use mÃ¡s de 500m CPU"
```

**Â¿QuÃ© significa "m" en CPU?**

`m` = millicores. 1000m = 1 CPU.

- `100m` = 0.1 CPU (10% de un core)
- `500m` = 0.5 CPU (50% de un core)

**Â¿Por quÃ© especificar esto?**

Sin limits, un Pod puede monopolizar todo el Node y matar a los demÃ¡s.
Sin requests, el Scheduler no sabe cuÃ¡nto espacio necesita.

```yaml
          livenessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 15
            periodSeconds: 10
```

**Â¿QuÃ© es livenessProbe?**

Pregunta: "Â¿EstÃ¡ VIVO el proceso?"

Si falla repetidamente â†’ Kubernetes MATA el Pod y crea uno nuevo.

**Â¿CuÃ¡ndo falla livenessProbe?**
- La aplicaciÃ³n se colgÃ³ (deadlock)
- La aplicaciÃ³n crasheÃ³ pero el proceso sigue
- El Pod no responde

**ParÃ¡metros**:
- `initialDelaySeconds: 15`: Esperar 15s antes del primer check (dar tiempo a que arranque)
- `periodSeconds: 10`: Revisar cada 10 segundos

```yaml
          readinessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 5
```

**Â¿QuÃ© es readinessProbe?**

Pregunta: "Â¿EstÃ¡ LISTO para recibir trÃ¡fico?"

Si falla â†’ Kubernetes DEJA de enviar trÃ¡fico al Pod (pero NO lo mata).

**Â¿CuÃ¡ndo falla readinessProbe?**
- La app estÃ¡ iniciando (cargando datos)
- La app perdiÃ³ conexiÃ³n a la base de datos
- La app estÃ¡ temporalmente ocupada

**Diferencia clave**:

| Probe | Si falla | Ejemplo de uso |
|-------|----------|----------------|
| liveness | Mata el Pod | App se colgÃ³, necesita reinicio |
| readiness | Deja de enviar trÃ¡fico | App estÃ¡ iniciando, no lista aÃºn |

**Ejemplo prÃ¡ctico**:

```
Pod arranca
â”‚
â”œâ”€â”€ 0s-5s: readiness failing (normal, estÃ¡ arrancando)
â”‚          â†’ No recibe trÃ¡fico
â”‚
â”œâ”€â”€ 5s: readiness passing
â”‚       â†’ Empieza a recibir trÃ¡fico
â”‚
â””â”€â”€ DespuÃ©s: liveness checking cada 10s
             â†’ Si falla 3 veces, reiniciar Pod
```

---

## service.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend
  namespace: voting-app
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 8080
  selector:
    app: frontend
```

**`type: LoadBalancer`**: Azure crearÃ¡ un Load Balancer real con IP pÃºblica.

**`port: 80`**: El puerto expuesto al mundo (lo que el usuario pone en el navegador).

**`targetPort: 8080`**: El puerto al que envÃ­a el trÃ¡fico (donde el contenedor escucha).

```
Usuario â”€â”€HTTP:80â”€â”€â–¶ Load Balancer â”€â”€:8080â”€â”€â–¶ Pod
```

**`selector: app: frontend`**: EnvÃ­a trÃ¡fico a TODOS los Pods con label `app: frontend`.

Kubernetes automÃ¡ticamente balancea entre todos los Pods que matchean.

---

## hpa.yaml (Horizontal Pod Autoscaler)

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: frontend-hpa
  namespace: voting-app
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: frontend
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
```

**Â¿QuÃ© hace el HPA?**

AutomÃ¡ticamente ajusta el nÃºmero de rÃ©plicas basado en mÃ©tricas.

**Flujo**:
```
1. HPA lee mÃ©tricas de CPU de todos los Pods del frontend
2. Calcula el promedio
3. Si promedio > 70% â†’ agregar rÃ©plicas
4. Si promedio < 70% â†’ quitar rÃ©plicas (pero nunca menos de minReplicas)
```

**Ejemplo**:

```
SituaciÃ³n: 2 pods, cada uno al 85% CPU
           Promedio = 85%
           Target = 70%
           
HPA calcula: necesito ~ 85/70 * 2 = 2.4 â†’ 3 pods

Resultado: HPA crea 1 pod mÃ¡s

Nueva situaciÃ³n: 3 pods, cada uno al ~57% CPU
                 Promedio = 57%
                 
HPA: OK, no harÃ© nada
```

**Â¿Por quÃ© min 2?**: Alta disponibilidad. Si uno muere, el otro sigue sirviendo mientras se recrea.

**Â¿Por quÃ© max 10?**: Prevenir runaway scaling. Si hay un bug que causa 100% CPU, no queremos crear infinitos pods.

---

## pdb.yaml (PodDisruptionBudget)

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: frontend-pdb
  namespace: voting-app
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: frontend
```

**Â¿QuÃ© es PDB?**

Una garantÃ­a de disponibilidad durante operaciones voluntarias (upgrades, drain, etc.).

**Â¿QuÃ© es una "disruption voluntaria"?**

- Admin drena un Node para mantenimiento
- Upgrade de Kubernetes
- Scaling down del cluster

**Sin PDB**:
```
Admin: kubectl drain node-1
Kubernetes: OK, mato todos los pods de node-1
            (incluyendo los 2 Ãºnicos frontends)
Usuarios: ERROR 503
```

**Con PDB**:
```
Admin: kubectl drain node-1
Kubernetes: Hay 2 frontends en node-1, pero PDB dice minAvailable=1
            Mato 1, espero a que otro estÃ© listo,
            luego mato el segundo
Usuarios: (ni se enteran)
```

**`minAvailable: 1`**: Siempre debe haber al menos 1 Pod disponible.

Alternativa: `maxUnavailable: 1` = mÃ¡ximo 1 puede estar no-disponible a la vez.
