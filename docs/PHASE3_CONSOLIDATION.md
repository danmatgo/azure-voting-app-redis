# üìö FASE 3: Consolidaci√≥n del Conocimiento
## Kubernetes Manifests y Deployment a AKS

---

## ‚úÖ Revisi√≥n de tus Archivos

| Archivo | Estado | Observaciones |
|---------|--------|---------------|
| `namespace.yaml` | ‚úÖ Perfecto | Namespace con labels apropiados |
| `configmap.yaml` | ‚úÖ Perfecto | Variables de configuraci√≥n externalizadas |
| `redis-deployment.yaml` | ‚úÖ Perfecto | Deployment + ClusterIP Service |
| `frontend-deployment.yaml` | ‚úÖ Perfecto | Rolling update, probes, resources |
| `frontend-service.yaml` | ‚úÖ Perfecto | LoadBalancer con port mapping correcto |
| `hpa.yaml` | ‚úÖ Perfecto | Autoscaling basado en CPU |

---

## üîß Cambios/Ajustes que Hiciste (Errores Resueltos)

### 1. Puerto del Container: 80 ‚Üí 8080

**Lo que cambiaste:**
```yaml
# T√∫ pusiste:
ports:
  - containerPort: 8080
livenessProbe:
  httpGet:
    port: 8080
```

**Por qu√© fue necesario:**
- La imagen base `python:3.11-slim` o tu app Flask probablemente escucha en 8080
- El Service mapea `port: 80` (externo) ‚Üí `targetPort: 8080` (container)
- Esto es patr√≥n com√∫n: usuarios acceden por 80, container usa puerto no-privilegiado

**Lecci√≥n aprendida**: Siempre verificar en qu√© puerto escucha la app antes de configurar probes.

---

### 2. Agregaste imagePullPolicy: Always

```yaml
imagePullPolicy: Always
```

**Por qu√© es importante:**
- Con tag `latest`, Docker puede usar imagen cacheada
- `Always` fuerza a verificar el registry cada vez
- Asegura que siempre tengas la versi√≥n m√°s reciente

| Policy | Comportamiento |
|--------|----------------|
| `Always` | Siempre verifica registry (usa m√°s bandwidth) |
| `IfNotPresent` | Usa cache si existe localmente |
| `Never` | Solo usa imagen local |

**En producci√≥n con SHA tags**: Usar√≠as `IfNotPresent` porque SHA es inmutable.

---

### 3. HPA Simplificado (solo CPU)

**T√∫ pusiste:**
```yaml
metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

**Removiste memory** - est√° bien porque:
- CPU es el indicador m√°s com√∫n de carga
- Memory en Python/Flask es m√°s estable
- Menos complejidad = menos posibles problemas

---

## üèóÔ∏è Arquitectura Kubernetes que Desplegaste

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      NAMESPACE: voting-app                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   ConfigMap     ‚îÇ          ‚îÇ      HPA (frontend-hpa)     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ voting-app-config‚îÇ          ‚îÇ  min:2 max:10 CPU:70%       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ           ‚îÇenv                               ‚îÇscale             ‚îÇ
‚îÇ           ‚ñº                                  ‚ñº                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ              DEPLOYMENT: frontend (replicas: 2)         ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                            ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Pod 1    ‚îÇ  ‚îÇ Pod 2    ‚îÇ  ‚Üê rolling update          ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ :8080    ‚îÇ  ‚îÇ :8080    ‚îÇ  ‚Üê maxSurge:1              ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚Üê maxUnavailable:0        ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ           ‚îÇ                                                      ‚îÇ
‚îÇ           ‚ñº                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                         ‚îÇ
‚îÇ  ‚îÇ SERVICE: frontend  ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ LoadBalancer                       ‚îÇ
‚îÇ  ‚îÇ port: 80           ‚îÇ      IP P√∫blica de Azure                ‚îÇ
‚îÇ  ‚îÇ targetPort: 8080   ‚îÇ                                         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                         ‚îÇ
‚îÇ           ‚îÇ                                                      ‚îÇ
‚îÇ           ‚îÇ env REDIS="redis"                                   ‚îÇ
‚îÇ           ‚ñº                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ              DEPLOYMENT: redis (replicas: 1)            ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                          ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Pod      ‚îÇ  ‚Üê stateless para pr√°ctica               ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ :6379    ‚îÇ  ‚Üê producci√≥n usar√≠a PVC                 ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                          ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ           ‚îÇ                                                      ‚îÇ
‚îÇ           ‚ñº                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                         ‚îÇ
‚îÇ  ‚îÇ SERVICE: redis     ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ ClusterIP (solo interno)           ‚îÇ
‚îÇ  ‚îÇ port: 6379         ‚îÇ      DNS: redis.voting-app.svc          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                         ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìñ Recapitulaci√≥n: ¬øQu√© significa cada cosa?

### Namespace

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: voting-app
```

| Concepto | Significado |
|----------|-------------|
| `Namespace` | Aislamiento l√≥gico dentro del cluster |
| Por qu√© usarlo | Separar recursos por proyecto/ambiente |
| Scope | Services, Pods, ConfigMaps son namespace-scoped |

**Comandos √∫tiles:**
```bash
kubectl get all -n voting-app     # Ver todo en el namespace
kubectl config set-context --current --namespace=voting-app  # Default
```

---

### ConfigMap

```yaml
kind: ConfigMap
data:
  TITLE: "Azure Voting App"
  VOTE1VALUE: "Terraform"
```

| Concepto | Significado |
|----------|-------------|
| `ConfigMap` | Almacena configuraci√≥n no sensible |
| `data` | Pares clave-valor |
| `envFrom` | Inyecta todas las keys como env vars |

**ConfigMap vs Secret:**
- ConfigMap: Configuraci√≥n visible (titles, URLs, feature flags)
- Secret: Datos sensibles (passwords, API keys) - base64 encoded

---

### Deployment

```yaml
kind: Deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    spec:
      containers:
        - name: frontend
          image: votingappdevacr.azurecr.io/azure-vote-front:latest
```

| Campo | Significado |
|-------|-------------|
| `replicas: 2` | Siempre mantener 2 pods corriendo |
| `selector.matchLabels` | C√≥mo el Deployment encuentra sus pods |
| `template` | Plantilla para crear pods |
| `strategy: RollingUpdate` | Actualizar gradualmente |
| `maxSurge: 1` | M√°ximo 1 pod extra durante update |
| `maxUnavailable: 0` | Siempre al menos 2 disponibles |

**Rolling Update en acci√≥n:**
1. Crea un pod nuevo (3 total)
2. Espera que est√© Ready
3. Termina un pod viejo (2 total)
4. Repite hasta completar

---

### Probes (Liveness y Readiness)

```yaml
livenessProbe:
  httpGet:
    path: /
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
```

| Probe | Pregunta | Si falla |
|-------|----------|----------|
| `liveness` | "¬øEst√° vivo?" | Kubernetes reinicia el pod |
| `readiness` | "¬øPuede recibir tr√°fico?" | Se remueve del Service |

| Par√°metro | Significado |
|-----------|-------------|
| `initialDelaySeconds` | Espera antes de primer check |
| `periodSeconds` | Cada cu√°nto verificar |
| `failureThreshold` | Cu√°ntos fallos antes de actuar |
| `timeoutSeconds` | M√°ximo tiempo de espera por respuesta |

---

### Resources (Requests y Limits)

```yaml
resources:
  requests:
    cpu: 100m
    memory: 128Mi
  limits:
    cpu: 500m
    memory: 256Mi
```

| Campo | Significado | Efecto |
|-------|-------------|--------|
| `requests` | M√≠nimo garantizado | Scheduler usa esto para ubicar pods |
| `limits` | M√°ximo permitido | Container es killed si excede |

**Unidades:**
- `100m` = 100 millicores = 0.1 CPU
- `128Mi` = 128 Mebibytes

---

### Service

```yaml
kind: Service
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 8080
  selector:
    app: frontend
```

| Campo | Significado |
|-------|-------------|
| `type: LoadBalancer` | Crea Azure Load Balancer con IP p√∫blica |
| `port: 80` | Puerto donde escucha el Service |
| `targetPort: 8080` | Puerto del container |
| `selector` | Qu√© pods reciben el tr√°fico |

**Tipos de Service:**
| Tipo | Acceso | Uso |
|------|--------|-----|
| `ClusterIP` | Solo interno | Backend, databases |
| `NodePort` | Puerto en cada nodo | Raro en cloud |
| `LoadBalancer` | IP p√∫blica | Frontend, APIs |

---

### HPA (Horizontal Pod Autoscaler)

```yaml
kind: HorizontalPodAutoscaler
spec:
  scaleTargetRef:
    kind: Deployment
    name: frontend
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          averageUtilization: 70
```

| Campo | Significado |
|-------|-------------|
| `scaleTargetRef` | Qu√© Deployment escalar |
| `minReplicas` | Nunca menos de 2 |
| `maxReplicas` | Nunca m√°s de 10 |
| `averageUtilization: 70` | Escala cuando promedio CPU > 70% |

**behavior:**
- `stabilizationWindowSeconds: 300` para scaleDown: Evita "flapping"
- `stabilizationWindowSeconds: 0` para scaleUp: Respuesta inmediata

---

## üêõ Errores Comunes y Soluciones (Para Entrevista)

### Error 1: ImagePullBackOff

```
STATUS: ImagePullBackOff
```

**Causas:**
1. Imagen no existe en el registry
2. No hay permisos de pull (ACR ‚Üí AKS)
3. Nombre/tag incorrecto

**Debug:**
```bash
kubectl describe pod <pod-name> -n voting-app
# Buscar secci√≥n Events
```

**Soluci√≥n:**
```bash
# Verificar que la imagen existe
az acr repository show-tags --name votingappdevacr --repository azure-vote-front

# Verificar role assignment
az role assignment list --assignee <aks-kubelet-id> --scope <acr-id>
```

---

### Error 2: CrashLoopBackOff

```
STATUS: CrashLoopBackOff
```

**Causas:**
1. La app crashea al iniciar
2. Puerto incorrecto (probe falla)
3. Variable de entorno faltante (ej: REDIS)

**Debug:**
```bash
kubectl logs <pod-name> -n voting-app
kubectl logs <pod-name> -n voting-app --previous  # Logs del crash anterior
```

---

### Error 3: Pending (Pod no arranca)

```
STATUS: Pending
```

**Causas:**
1. No hay nodos con recursos suficientes
2. Node pool en scaling
3. PVC pending (si usa storage)

**Debug:**
```bash
kubectl describe pod <pod-name> -n voting-app
# Buscar: "Insufficient cpu" o "Insufficient memory"
```

---

### Error 4: Service no tiene EXTERNAL-IP

```
NAME       TYPE           EXTERNAL-IP   PORT(S)
frontend   LoadBalancer   <pending>     80:32100/TCP
```

**Causas:**
1. Azure LB a√∫n provisionando (esperar ~2 min)
2. Cuota de IPs p√∫blicas excedida
3. NSG bloqueando

**Soluci√≥n:**
```bash
kubectl get svc frontend -n voting-app -w  # Esperar
kubectl describe svc frontend -n voting-app  # Ver eventos
```

---

## üé§ Preguntas de Entrevista - Kubernetes

### B√°sicas

**P: ¬øCu√°l es la diferencia entre un Pod y un Deployment?**
> "Un Pod es la unidad m√≠nima - un contenedor o grupo que comparten storage y red. Pero los Pods son ef√≠meros, si mueren no regresan. Un Deployment es un controlador que mantiene un n√∫mero deseado de Pods, los recrea si fallan, y maneja rolling updates. En producci√≥n siempre uso Deployments, nunca Pods directamente."

**P: ¬øPara qu√© sirve un Service en Kubernetes?**
> "Los Pods tienen IPs que cambian cuando se recrean. El Service da una IP estable y nombre DNS, y hace load balancing entre los Pods. Por ejemplo, mi frontend se conecta al Service 'redis', no directamente a los Pods - si Redis se reinicia con nueva IP, el frontend sigue funcionando."

**P: ¬øQu√© es Rolling Update y por qu√© es importante?**
> "Es la estrategia de deployment por defecto. Reemplaza pods gradualmente: crea uno nuevo, espera que est√© healthy, luego termina uno viejo. As√≠ no hay downtime. Configuro maxSurge y maxUnavailable para controlar la velocidad. En mi proyecto us√© maxUnavailable:0 para garantizar siempre 2 pods disponibles."

### Intermedias

**P: ¬øCu√°ndo usar√≠as Liveness vs Readiness probe?**
> "Liveness detecta si la app est√° colgada - si falla, Kubernetes reinicia el pod. Readiness detecta si puede recibir tr√°fico - si falla, se remueve del Service pero sigue corriendo. Por ejemplo, durante un deployment nuevo, el pod puede estar vivo pero cargando datos, entonces liveness pasa pero readiness no hasta que termine."

**P: ¬øQu√© pasa si no defines resource requests/limits?**
> "Sin requests, el scheduler no sabe cu√°ntos recursos necesita el pod, puede sobrecargar nodos. Sin limits, un pod puede consumir toda la CPU/memoria del nodo, afectando otros pods. Es best practice siempre definir ambos - requests para scheduling, limits para protecci√≥n."

**P: ¬øC√≥mo debuggeas un pod que no arranca?**
> "Primero `kubectl describe pod` para ver eventos - puede ser ImagePullBackOff, Pending por recursos, o error de scheduling. Luego `kubectl logs` para ver output de la app. Si crashea, `kubectl logs --previous` muestra logs del crash anterior. Tambi√©n verifico `kubectl get events` para ver problemas a nivel cluster."

### Avanzadas

**P: ¬øC√≥mo implementar√≠as blue-green deployment?**
> "Tendr√≠a dos Deployments: blue (producci√≥n actual) y green (nueva versi√≥n). Ambos corren simult√°neamente. El Service apunta a blue. Despu√©s de validar green, cambio el selector del Service para apuntar a green. Si hay problemas, cambio de vuelta a blue instant√°neamente. Es m√°s recursos pero rollback inmediato."

**P: ¬øQu√© son Network Policies y cu√°ndo las usar√≠as?**
> "Son firewalls a nivel de Pod. Por defecto, todos los pods pueden comunicarse entre s√≠. Con Network Policies, puedo restringir - por ejemplo, solo el frontend puede conectarse a Redis, nada m√°s. Las usar√≠a en producci√≥n para defense in depth, especialmente en clusters multi-tenant."

**P: ¬øC√≥mo manejar√≠as secrets en Kubernetes?**
> "Kubernetes Secrets est√°n en base64, no encriptados. Para producci√≥n integro con Azure Key Vault usando el CSI driver o AAD Pod Identity. Los secrets se montan como archivos o variables, y la rotaci√≥n es autom√°tica. Nunca guardo secrets en Git ni en ConfigMaps."

---

## üîë Keywords para la Entrevista

| Keyword | C√≥mo usarla |
|---------|-------------|
| **Desired state** | "Kubernetes mantiene el estado deseado autom√°ticamente" |
| **Self-healing** | "Si un pod muere, el Deployment lo recrea" |
| **Rolling update** | "Zero downtime deployments con rolling update" |
| **Declarative** | "Defino QU√â quiero, no C√ìMO hacerlo" |
| **Pod anti-affinity** | "Distribuyo pods en diferentes nodos para HA" |
| **Resource quotas** | "Limito recursos por namespace para control de costos" |
| **Labels y selectors** | "Todo en K8s se conecta mediante labels" |
| **Horizontal scaling** | "HPA escala pods, Cluster Autoscaler escala nodos" |

---

## üìã Comandos que Ejecutaste

```bash
# Conectar a AKS
az aks get-credentials --resource-group votingapp-dev-rg --name votingapp-dev-aks

# Aplicar manifests en orden
kubectl apply -f namespace.yaml
kubectl apply -f configmap.yaml
kubectl apply -f redis-deployment.yaml
kubectl apply -f frontend-deployment.yaml
kubectl apply -f frontend-service.yaml
kubectl apply -f hpa.yaml

# Verificar
kubectl get all -n voting-app
kubectl get svc frontend -n voting-app  # Ver IP p√∫blica

# Debug √∫til
kubectl describe pod <name> -n voting-app
kubectl logs <pod-name> -n voting-app
kubectl exec -it <pod-name> -n voting-app -- /bin/sh
```

---

## ‚úÖ Checklist Conocimiento Fase 3

- [ ] Puedo explicar Pod vs Deployment vs Service
- [ ] Entiendo rolling update y sus par√°metros
- [ ] S√© la diferencia entre Liveness y Readiness probes
- [ ] Puedo explicar ClusterIP vs LoadBalancer
- [ ] Entiendo requests vs limits y por qu√© son importantes
- [ ] S√© debuggear ImagePullBackOff y CrashLoopBackOff
- [ ] Puedo explicar HPA y cu√°ndo usarlo
- [ ] Entiendo c√≥mo funciona el DNS interno de Kubernetes

---

## üéì Resumen: Lo que Puedes Decir en la Entrevista

> "En mi √∫ltimo proyecto implement√© una aplicaci√≥n multi-tier en AKS. Us√© Deployments con rolling update strategy - configur√© maxUnavailable:0 para garantizar zero downtime. Cada pod tiene liveness y readiness probes; el liveness reinicia pods colgados, el readiness previene enviar tr√°fico a pods que a√∫n est√°n iniciando. Para el backend us√© ClusterIP porque solo necesita acceso interno, y LoadBalancer para el frontend. Tambi√©n configur√© HPA para escalar autom√°ticamente basado en CPU al 70%, con min 2 r√©plicas para HA y max 10 para controlar costos. Todo declarativo, versionado en Git, y desplegado con kubectl apply."
